import math
from dataclasses import dataclass, asdict
from enum import Enum
from typing import List, Dict, Any

class Action(Enum):
    """Enumeration of possible supervisor interventions."""
    ALLOW = "ALLOW"  # Allow the agent's output as is
    WARN = "WARN"    # Log a warning but allow the output
    CORRECT = "CORRECT" # Attempt to correct the output
    ESCALATE = "ESCALATE" # Escalate the issue for human review

@dataclass
class AgentState:
    """Represents the state of the agent being supervised at a point in time."""
    quality_score: float
    error_count: int
    resource_usage: float
    task_progress: float
    drift_score: float = 0.0 # A score from 0.0 (no drift) to 1.0 (high drift)

    def is_terminal(self) -> bool:
        """Determines if the state is a terminal state (e.g., task complete or failed)."""
        return self.task_progress >= 1.0 or self.quality_score <= 0.1

class ExpectimaxAgent:
    """A supervisor agent that uses the Expectimax algorithm to choose interventions."""

    def __init__(self, depth: int = 2, weights: Dict[str, float] = None):
        """
        Initializes the Expectimax agent.
        Args:
            depth: The maximum depth for the search tree.
            weights: A dictionary of weights for the evaluation function.
        """
        self.depth = depth
        if weights is None:
            # Default weights if none are provided
            self.weights = {
                "quality_score": 0.30,
                "task_progress": 0.10,
                "inv_drift_score": 0.20,
                "inv_error_count": 0.30,
                "inv_resource_usage": 0.10,
            }
        else:
            self.weights = weights

        # Normalize weights to sum to 1, ensuring relative importance is maintained
        total_weight = sum(self.weights.values())
        if total_weight > 0:
            for key in self.weights:
                self.weights[key] /= total_weight


    def _evaluate_state(self, state: AgentState) -> Dict[str, Any]:
        """
        The evaluation function for a given state.
        Returns a dictionary with the score and the features used.
        """
        if state.is_terminal():
            score = 1.0 if state.task_progress >= 1.0 else 0.0
            return {"score": score, "features": {"is_terminal": True, "progress": state.task_progress}}

        # Transform features to be "higher is better" in a 0-1 range
        features = {
            "quality_score": state.quality_score,
            "task_progress": min(1.0, state.task_progress),
            "inv_drift_score": 1.0 - state.drift_score,
            "inv_error_count": 1.0 / (1.0 + state.error_count),
            "inv_resource_usage": 1.0 - state.resource_usage,
        }

        # Calculate weighted sum
        score = sum(self.weights[key] * features[key] for key in self.weights)

        return {"score": score, "features": features}

    def _get_possible_actions(self, state: AgentState) -> List[Action]:
        """Returns a list of sensible actions based on the current state."""

        # If there are too many errors, we must escalate.
        if state.error_count >= 3:
            return [Action.ESCALATE]

        actions = []
        if state.quality_score >= 0.9:
            # If quality is very high, only allow or warn. No need to correct.
            actions.extend([Action.ALLOW, Action.WARN])
        elif state.quality_score < 0.4:
            # If quality is very low, allowing it is not an option.
            actions.extend([Action.CORRECT, Action.ESCALATE])
        else:
            # In the middle range, all actions are on the table.
            actions.extend([Action.ALLOW, Action.WARN, Action.CORRECT, Action.ESCALATE])

        return list(set(actions)) # Return unique actions

    def _get_action_outcomes(self, state: AgentState, action: Action) -> List[tuple[float, AgentState]]:
        """
        Returns a list of possible outcomes for a given action, each with a probability.
        Returns: A list of (probability, next_state) tuples.
        """
        outcomes = []
        base_state = AgentState(
            quality_score=state.quality_score,
            error_count=state.error_count,
            resource_usage=state.resource_usage,
            task_progress=state.task_progress
        )
        # All actions increase resource usage slightly
        base_state.resource_usage = min(1.0, base_state.resource_usage + 0.05)

        if action == Action.ALLOW:
            # 80% chance quality degrades slightly, 20% chance it stays the same
            state_degrade = AgentState(**base_state.__dict__)
            state_degrade.quality_score *= 0.95
            state_degrade.task_progress += 0.1
            outcomes.append((0.8, state_degrade))

            state_same = AgentState(**base_state.__dict__)
            state_same.task_progress += 0.1
            outcomes.append((0.2, state_same))

        elif action == Action.WARN:
            # Warning has a small cost and slightly less progress
            state_warn = AgentState(**base_state.__dict__)
            state_warn.task_progress += 0.05
            state_warn.resource_usage = min(1.0, state_warn.resource_usage + 0.1)
            outcomes.append((1.0, state_warn))

        elif action == Action.CORRECT:
            # 70% chance of successful correction, 30% chance of failure
            state_success = AgentState(**base_state.__dict__)
            state_success.resource_usage = min(1.0, state_success.resource_usage + 0.15)
            if state.quality_score < 0.85:
                quality_improvement = 0.4 * (1 - state.quality_score)
                state_success.quality_score = min(1.0, state.quality_score + quality_improvement)
                state_success.error_count = max(0, state.error_count - 1)
            state_success.task_progress += 0.1
            outcomes.append((0.7, state_success))

            state_fail = AgentState(**base_state.__dict__)
            state_fail.resource_usage = min(1.0, state_fail.resource_usage + 0.15)
            # No improvement on failure, just cost
            state_fail.task_progress += 0.02
            outcomes.append((0.3, state_fail))

        elif action == Action.ESCALATE:
            # Escalation is a deterministic major setback
            state_escalate = AgentState(**base_state.__dict__)
            state_escalate.task_progress *= 0.2
            state_escalate.quality_score *= 0.1
            state_escalate.error_count += 1
            outcomes.append((1.0, state_escalate))

        return outcomes

    def expectimax(self, state: AgentState, depth: int, agent_turn: bool, alpha: float, beta: float) -> float:
        """
        The core Expectimax algorithm with alpha-pruning for the maximizer.
        """
        if depth == 0 or state.is_terminal():
            return self._evaluate_state(state)

        if agent_turn:  # Maximizer node
            max_eval = -math.inf
            for action in self._get_possible_actions(state):
                # The value of an action is the expected value of its outcomes.
                # We pass our current alpha and beta to the chance node.
                expected_value = self.expectimax(state, depth, False, alpha, beta)
                max_eval = max(max_eval, expected_value)
                alpha = max(alpha, max_eval)
                # Note: Beta pruning is not applicable in the maximizer for Expectimax
                # because we need to know the exact expected value from the chance node,
                # not just if it's above a certain threshold. However, we can pass alpha
                # down to potentially prune in deeper maximizer nodes.
            return max_eval

        else:  # Chance node
            total_expected_value = 0
            # For a given action (which is implicit here, this logic is flawed),
            # we would get its outcomes. This needs restructuring.
            # Let's assume this node calculates the value for a single, preceding action.

            # This requires a significant restructure. Let's simplify the logic to be more direct.
            # The get_best_action will orchestrate the calls.
            pass # Logic will be handled in get_best_action for clarity.

    def _get_action_value(self, state: AgentState, action: Action, depth: int, alpha: float, beta: float) -> float:
        """Calculates the expected value of a single action."""
        if depth == 0 or state.is_terminal():
            return self._evaluate_state(state)['score']

        outcomes = self._get_action_outcomes(state, action)
        expected_value = 0
        for probability, next_state in outcomes:
            # Find the value of the best action from the next state.
            max_next_eval = -math.inf
            for next_action in self._get_possible_actions(next_state):
                # This is where a recursive call with pruning would happen.
                # For simplicity in this step, we'll call a non-pruning version.
                # A full alpha-beta implementation would pass new alpha/beta values here.
                evaluation = self._get_action_value(next_state, next_action, depth - 1, alpha, beta)
                if evaluation > max_next_eval:
                    max_next_eval = evaluation
            expected_value += probability * max_next_eval
        return expected_value

    def get_best_action(self, state: AgentState) -> Dict[str, Any]:
        """
        Finds the best action to take from the current state using Expectimax with pruning.
        """
        best_score = -math.inf
        best_action = None
        considered_actions = []
        alpha = -math.inf
        beta = math.inf

        for action in self._get_possible_actions(state):
            # The old expectimax call was incorrect.
            # A better structure is to have a helper that evaluates an action.
            score = self._get_action_value(state, action, self.depth, alpha, beta)
            considered_actions.append({"action": action.value, "score": score})

            if score > best_score:
                best_score = score
                best_action = action

            # Update alpha for pruning subsequent actions at this level
            alpha = max(alpha, best_score)

        best_action = best_action or Action.ALLOW

        return {
            "best_action": best_action,
            "best_score": best_score,
            "considered_actions": sorted(considered_actions, key=lambda x: x['score'], reverse=True),
            "state_evaluated": state
        }


    def _get_action_value_with_trace(self, state: AgentState, action: Action, depth: int) -> tuple[float, Dict[str, Any]]:
        """
        Calculates the expected value of a single action and returns a trace.
        """
        evaluation = self._evaluate_state(state)

        if depth == 0 or state.is_terminal():
            trace = {
                "name": f"Terminal (Score: {evaluation['score']:.2f})",
                "state": asdict(state),
                "evaluation": evaluation,
                "children": []
            }
            return evaluation['score'], trace

        outcomes = self._get_action_outcomes(state, action)
        expected_value = 0
        outcome_traces = []

        for probability, next_state in outcomes:
            # Find the value of the best action from the next state.
            max_next_eval = -math.inf
            best_next_action_trace = {}

            for next_action in self._get_possible_actions(next_state):
                # Recursive call to get the value and trace of the sub-problem
                evaluation_score, sub_trace = self._get_action_value_with_trace(next_state, next_action, depth - 1)

                if evaluation_score > max_next_eval:
                    max_next_eval = evaluation_score
                    best_next_action_trace = {
                        "name": f"Action: {next_action.value} (Value: {evaluation_score:.2f})",
                        "state": asdict(next_state),
                        "evaluation": self._evaluate_state(next_state),
                        "children": sub_trace.get('children', [])
                    }

            outcome_node = {
                "name": f"Outcome (Prob: {probability:.2f})",
                "probability": probability,
                "state": asdict(next_state),
                "children": [best_next_action_trace] if best_next_action_trace else []
            }
            outcome_traces.append(outcome_node)
            expected_value += probability * max_next_eval

        return expected_value, {"children": outcome_traces}


    def get_best_action_with_trace(self, state: AgentState) -> Dict[str, Any]:
        """
        Finds the best action and returns a full trace of the decision process.
        """
        best_score = -math.inf
        best_action = None

        root_trace = {
            "name": f"Initial State (Score: {self._evaluate_state(state)['score']:.2f})",
            "state": asdict(state),
            "evaluation": self._evaluate_state(state),
            "children": []
        }

        for action in self._get_possible_actions(state):
            score, trace_children = self._get_action_value_with_trace(state, action, self.depth)

            action_node = {
                "name": f"Action: {action.value} (Exp. Score: {score:.2f})",
                "action": action.value,
                "score": score,
                "children": trace_children.get('children', [])
            }
            root_trace["children"].append(action_node)

            if score > best_score:
                best_score = score
                best_action = action

        best_action = best_action or Action.ALLOW

        # Sort children by score for readability
        root_trace["children"].sort(key=lambda x: x['score'], reverse=True)

        return {
            "best_action": best_action,
            "best_score": best_score,
            "trace": root_trace
        }
